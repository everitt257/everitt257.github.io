<!DOCTYPE html>
<!-- custom.css -->
<link rel="stylesheet", href="/css/custom.css">

<!-- mathjax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    TeX: {
      equationNumbers: {
        autoNumber: "AMS"
      }
    },
    tex2jax: {
      inlineMath: [ ['$','$'] ],
      displayMath: [ ['$$','$$'] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre']
    }
  });
</script>
<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

<!-- mermaid -->
<script src="/js/mermaid.min.js"></script>
<script>mermaid.initialize({startOnLoad:true});</script>

<!-- body -->
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.5.0 -->
<title>Discussion of generation models, sequential generation | Everitt’s blog</title>
<meta name="generator" content="Jekyll v3.8.2" />
<meta property="og:title" content="Discussion of generation models, sequential generation" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Previously I’ve read about R2RT’s post regarding sequential generation and discrete embeddings. I thought I’ll write something to remind myself the details of this post and to compare it with other generation models." />
<meta property="og:description" content="Previously I’ve read about R2RT’s post regarding sequential generation and discrete embeddings. I thought I’ll write something to remind myself the details of this post and to compare it with other generation models." />
<link rel="canonical" href="http://localhost:4000/blog/2018/06/22/discrete_embeddings.html" />
<meta property="og:url" content="http://localhost:4000/blog/2018/06/22/discrete_embeddings.html" />
<meta property="og:site_name" content="Everitt’s blog" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2018-06-22T00:00:00+08:00" />
<script type="application/ld+json">
{"description":"Previously I’ve read about R2RT’s post regarding sequential generation and discrete embeddings. I thought I’ll write something to remind myself the details of this post and to compare it with other generation models.","@type":"BlogPosting","url":"http://localhost:4000/blog/2018/06/22/discrete_embeddings.html","headline":"Discussion of generation models, sequential generation","dateModified":"2018-06-22T00:00:00+08:00","datePublished":"2018-06-22T00:00:00+08:00","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/blog/2018/06/22/discrete_embeddings.html"},"@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/main.css"><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="Everitt's blog" /></head>
<body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/">Everitt&#39;s blog</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/about/">About</a><a class="page-link" href="/category/blog.html">Blog</a><a class="page-link" href="/category/work.html">Work</a><a class="page-link" href="/tags.html">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Discussion of generation models, sequential generation</h1>
    <p class="post-meta">
      <time class="dt-published" datetime="2018-06-22T00:00:00+08:00" itemprop="datePublished">Jun 22, 2018
      </time></p>
  </header>

  <!-- Tags -->
  <ul class="tags">
    
      <li><a href="/tags#deep learning" class="tag">deep learning</a></li>
    
      <li><a href="/tags#algorithm" class="tag">algorithm</a></li>
    
  </ul>

  <!-- Body -->
  <div class="post-content e-content" itemprop="articleBody">
    <p>Previously I’ve read about R2RT’s post regarding sequential generation and discrete embeddings. I thought I’ll write something to remind myself the details of this post and to compare it with other generation models.</p>

<p>Discrete embedding can prevent overfitting as we’ve seen it in my other post. It can also communicate fuzzy ideas with concrete symbols. This means (1) we can create a language model over the embeddings, which give us give RNN-based generation of internal embeddings, and (2) index sub-parts of the embeddings, whichi gives us access to the search techniques that go beyong cosine similarity, such as phrase search.</p>

<p>We’ll use MNIST dataset for illustration. The ultimate goal is as described: given a sequence of imcomplete features, can we we find subsequences of consecutive digits that have these features?</p>

<p class="mycenter"><img src="http://om1hdizoc.bkt.clouddn.com/18-6-22/1743208.jpg" alt="" /></p>

<p>Notice the original’s post’s solution to this question is not perfect. You might want to view the original post at <a href="https://r2rt.com/deconstruction-with-discrete-embeddings.html">here</a>.</p>

<h3 id="embeddings">Embeddings</h3>
<p>Why discrete embeddings not real embeddings like w2v? While real embeddings may capture more details regarding the dataset, such as width, heights, angles etc, the discrete embeddings allows user to apply explicit reasoning and algoirhtms over the data and it helps with overfitting. But we can always use both, for example, a mixture of both real and discrete embeddings during our training.</p>

<h3 id="autoencoder">Autoencoder</h3>
<p>The original post talks about building an autoencoder with discrete embeddings with one digit in the latent variable unused. We are just going to conclude that discrete embeddings are sufficient for reconstructing the original post and 560 zeros and 80 ones are sufficient in communicating during the reconstruction.</p>

<h3 id="sequential-generator">Sequential Generator</h3>
<p>The real beauty of this post comes when it tries to reconstruct images with RNN. To illustrate this, we will show the original code.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">imgs_to_indices</span><span class="p">(</span><span class="n">imgs</span><span class="p">):</span>
    <span class="n">embs</span> <span class="o">=</span> <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">g</span><span class="p">[</span><span class="s">'embedding'</span><span class="p">],</span> <span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="n">g</span><span class="p">[</span><span class="s">'x'</span><span class="p">]:</span> <span class="n">imgs</span><span class="p">,</span> <span class="n">g</span><span class="p">[</span><span class="s">'stochastic'</span><span class="p">]:</span> <span class="bp">False</span><span class="p">})</span> <span class="c">#[n, 80, 8]</span>
    <span class="n">idx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">embs</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span> <span class="c">#[n, 80]</span>
    <span class="n">res</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">img</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">imgs</span><span class="p">)):</span>
        <span class="n">neuron_perm</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">permutation</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">80</span><span class="p">)))</span> <span class="c">#order of neurons we will present</span>
        <span class="n">res</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">idx</span><span class="p">[</span><span class="n">img</span><span class="p">][</span><span class="n">neuron_perm</span><span class="p">]</span> <span class="o">+</span> <span class="n">neuron_perm</span> <span class="o">*</span> <span class="mi">8</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">res</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">gen_random_neuron_batch</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">mnist</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">next_batch</span><span class="p">(</span><span class="n">n</span><span class="p">)</span> <span class="c"># [n, 784]</span>
    <span class="n">res</span> <span class="o">=</span> <span class="n">imgs_to_indices</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">res</span><span class="p">[:,:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">res</span><span class="p">[:,</span><span class="mi">1</span><span class="p">:]</span>
</code></pre></div></div>
<p>As explained in the original post, there are no hierarchical structure regarding the order of how of how we query our discrete embeddings. Therefore we randomize the order and produce such random neuron batch. Taking a closer look this code:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">neuron_perm</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">permutation</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">80</span><span class="p">)))</span> <span class="c">#order of neurons we will present</span>
<span class="n">res</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">idx</span><span class="p">[</span><span class="n">img</span><span class="p">][</span><span class="n">neuron_perm</span><span class="p">]</span> <span class="o">+</span> <span class="n">neuron_perm</span> <span class="o">*</span> <span class="mi">8</span><span class="p">)</span>
</code></pre></div></div>

<p>First we are adding randomness to the order. Secondly we are preserving the indexes of the neuron. Without the digit 8, we’ll have duplicates and the order will be messed up. Therefore we must use 640 to index each our neuron in each example.</p>

<p>Another thing to notice is that since randomness is added, we are essentially transforming the discrete embeddings trained in the autoencoder to randomized <strong>sequential</strong> embeddings. And then applied to RNN. This is essential since two model (autoencoder and RNN) uses two different kind of embeddings.</p>

<p>Supply with mermaid graph here:</p>

<style>
.mycenter {
    text-align:center;
}
</style>


  </div>
  
  <!-- Related posts -->
  
  
  
    <div class="row related-posts">
      <h2 class="text-center" style="font-family: initial">Related blog posts:</h2>
      <div class="medium-12 small-12 columns">
        
          

           <h3>
            <a href="http://localhost:4000/blog/2018/07/05/VAE_GAN.html">
              A comparison between VAE and GAN
            </a>
           </h3>

          
        
          
        
          

           <h3>
            <a href="http://localhost:4000/blog/2018/06/04/ternary-and-one-hot-neuron.html">
              承接Binary, 拥抱ternary和one-hot-neurons
            </a>
           </h3>

          
        
      </div>
    </div>
  


  <!-- Disqus --><a class="u-url" href="/blog/2018/06/22/discrete_embeddings.html" hidden></a>
</article>


<script id="dsq-count-scr" src="//everitt257.disqus.com/count.js" async></script>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">Everitt&#39;s blog</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">Everitt&#39;s blog</li><li><a class="u-email" href="mailto:everitt257@gmail.com">everitt257@gmail.com</a></li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"><li><a href="https://github.com/everitt257"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg> <span class="username">everitt257</span></a></li></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>This site fancies machine learning and problems in engineerning in general.</p>
      </div>
    </div>

  </div>

</footer>
</body>

</html>
