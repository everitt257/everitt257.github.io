<!DOCTYPE html>
<!-- custom.css -->
<link rel="stylesheet", href="/css/custom.css">

<!-- mathjax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    TeX: {
      equationNumbers: {
        autoNumber: "AMS"
      }
    },
    tex2jax: {
      inlineMath: [ ['$','$'] ],
      displayMath: [ ['$$','$$'] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre']
    }
  });
</script>
<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

<!-- mermaid -->
<script src="/js/mermaid.min.js"></script>
<script>mermaid.initialize({startOnLoad:true});</script>

<!-- body -->
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.5.0 -->
<title>Hidden Markov Chain Model | Everitt’s blog</title>
<meta name="generator" content="Jekyll v3.8.2" />
<meta property="og:title" content="Hidden Markov Chain Model" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Hidden Markov Model The whole reason I’m recording this is because RF uses some of properties of Hidden Markov Model or the Kalman-filter model to estimate the hidden state. Notice the difference between HMM and KF is that one is discrete time spaced and the other continuous time spaced. The Bayesian network The Bayesian network is merely a way to graphically think conditional probability." />
<meta property="og:description" content="Hidden Markov Model The whole reason I’m recording this is because RF uses some of properties of Hidden Markov Model or the Kalman-filter model to estimate the hidden state. Notice the difference between HMM and KF is that one is discrete time spaced and the other continuous time spaced. The Bayesian network The Bayesian network is merely a way to graphically think conditional probability." />
<link rel="canonical" href="http://localhost:4000/blog/2018/02/23/hidden-markov.html" />
<meta property="og:url" content="http://localhost:4000/blog/2018/02/23/hidden-markov.html" />
<meta property="og:site_name" content="Everitt’s blog" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2018-02-23T00:00:00+08:00" />
<script type="application/ld+json">
{"description":"Hidden Markov Model The whole reason I’m recording this is because RF uses some of properties of Hidden Markov Model or the Kalman-filter model to estimate the hidden state. Notice the difference between HMM and KF is that one is discrete time spaced and the other continuous time spaced. The Bayesian network The Bayesian network is merely a way to graphically think conditional probability.","@type":"BlogPosting","url":"http://localhost:4000/blog/2018/02/23/hidden-markov.html","headline":"Hidden Markov Chain Model","dateModified":"2018-02-23T00:00:00+08:00","datePublished":"2018-02-23T00:00:00+08:00","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/blog/2018/02/23/hidden-markov.html"},"@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/main.css"><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="Everitt's blog" /></head>
<body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/">Everitt&#39;s blog</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/about/">About</a><a class="page-link" href="/category/blog.html">Blog</a><a class="page-link" href="/category/work.html">Work</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Hidden Markov Chain Model</h1>
    <p class="post-meta">
      <time class="dt-published" datetime="2018-02-23T00:00:00+08:00" itemprop="datePublished">Feb 23, 2018
      </time></p>
  </header>

  <!-- Tags -->
  <ul class="tags">
    
      <li><a href="/tags#algorithm" class="tag">algorithm</a></li>
    
  </ul>

  <!-- Body -->
  <div class="post-content e-content" itemprop="articleBody">
    <h1 id="hidden-markov-model">Hidden Markov Model</h1>
<p>The whole reason I’m recording this is because RF uses some of properties of Hidden Markov Model or the Kalman-filter model to estimate the hidden state. <em>Notice the difference between HMM and KF is that one is discrete time spaced and the other continuous time spaced</em>.</p>
<h2 id="the-bayesian-network">The Bayesian network</h2>
<p>The Bayesian network is merely a way to graphically think conditional probability.</p>

<p><img src="http://om1hdizoc.bkt.clouddn.com/18-1-20/61211680.jpg" alt="" /></p>

<h2 id="the-d-seperateion">The D-seperateion:</h2>
<p>The D-seperation is used for trackable inference.
<img src="http://om1hdizoc.bkt.clouddn.com/18-1-20/14462958.jpg" alt="" />
Also notice that the D-seperation can be used in Bayesian network as well. Check video link here:
<a href="https://www.youtube.com/watch?v=zJIK5uOyJi0&amp;list=PLD0F06AA0D2E8FFBA&amp;index=92">D seperation</a></p>
<h2 id="the-forward-backward-procedure">The Forward-backward procedure</h2>
<p>The algorithm that reveals $p(z_k \mid x)$ uses the forward and backward algorithm.
<img src="http://om1hdizoc.bkt.clouddn.com/18-1-20/21967713.jpg" alt="" />
By using the idea of d-seperation, we can reduce $p(z_k \mid x)$ into something simpler as above.</p>
<blockquote>
  <p>In one online discussion, someone called this term $p(z_k \mid x)$ as filtering.<a href="https://stats.stackexchange.com/questions/183118/difference-between-hidden-markov-models-and-particle-filter-and-kalman-filter">[1]</a></p>
</blockquote>

<p>The first term $p(x_{k+1:n} \mid z_k)$ is the so called for <strong>backward procedure</strong>. The second term $p(z_k, x_{1:k})$ is the <strong>forward procedure</strong>.</p>
<h3 id="the-forward-algorithm">The forward algorithm</h3>
<p>The step-back trick:
<script type="math/tex">p(z_k, x_{1:k}) = \sum^m_{z_{k-1} = 1} p(z_k, z_{k-1}, x_{1:k})</script>
By the tree structure and the d-seperation principle and with this trick in mind:
<img src="http://om1hdizoc.bkt.clouddn.com/18-1-20/19958631.jpg" alt="" />
The result is thus:
<img src="http://om1hdizoc.bkt.clouddn.com/18-1-20/87529527.jpg" alt="" /></p>
<ul>
  <li>The first equality comes from the stepback trick.</li>
  <li>The second equality comes from the chain rule of probability <strong>and</strong> the D-seperation principle.</li>
</ul>

<p>we have the following equation:
<script type="math/tex">p(z_k, x_{1:k}) = \sum_{z_{k-1}} p(x_k \mid z_k) p(z_k \mid z_{k-1}) p(z_{k-1}, x_{1:k-1})</script>
The first term is the <strong>emission probability</strong>, the second term is the <strong>transition probability</strong>. As you can see, this is a <strong>recursive structure</strong>. Normally, the emission and transition are given. They correspond to A and C.</p>

<h4 id="application-finding-observed-sequence-probability">Application: finding observed sequence probability</h4>
<p>Also, the forward pass can be summed up to calculate the oberserved sequence probability. There’s an alternative(in Andrew’s note) which is more computationally expensive, thus we use this approach instead.
<img src="http://om1hdizoc.bkt.clouddn.com/18-1-20/75267330.jpg" alt="" /></p>
<h3 id="the-backward-algorithm">The backward algorithm</h3>
<p>Similar to the forward algorithm, the backward algorithm also make use of the recursive structure, which is the heart of dynamic programming.
<img src="http://om1hdizoc.bkt.clouddn.com/18-1-20/5769210.jpg" alt="" /></p>
<h2 id="the-viterbi-algorithm">The Viterbi algorithm</h2>
<p>One of the most common queries of a Hidden Markov Model is to ask what was the most likely series of states $\vec{z} \in S^T$ given an observed series of outputs $\vec{x} \in V^T$. One potential application of viterbi algorithm can be used in hand writing recognition.</p>
<h3 id="the-deduction-proof">The deduction proof</h3>
<p>The deduction is as follows:</p>

<p>Given: $x = x_{1:n}$ and $z = z_{1:n}$, the goal is to compute <script type="math/tex">z^* = \text{arg max}_z p(z \mid x)</script>
Notice that
<script type="math/tex">\text{arg max}_{z_{1:n}} p(z \mid x) = \text{arg max}_{z_{1:n}} p(z,x)</script>
also let
<script type="math/tex">u_k(z_k) = max_{z_{1:k-1}}p(z_{1:k},x_{1:k})</script>
along with the fact that
<img src="http://om1hdizoc.bkt.clouddn.com/18-1-21/29244779.jpg" alt="" />
also with help of chain rule of probability and the D-seperation in our network. We can come to another recursion structure:
 <img src="http://om1hdizoc.bkt.clouddn.com/18-1-21/70736219.jpg" alt="" />
 This essentially say that in order to reveal the hidden sequence of state. We need to calculate every possible sequence’s probability value recursively. In another word, the $u_k(z_k)$ term means the optimal step to take at time step k. If you know the end hidden state in advance then you can iterate all path’s probability value to find the optimal choice. However if k at current time step is just another intermediate step, one would need to calculate all possible hidden state’s optimal choices.  Note that in this setting we assume $A$ the <strong>state transition probability matrix</strong> and $B$ the <strong>output emission probability matrix</strong> is known. Normally one would need to run ML to estimate these parameters.</p>

<h2 id="the-em-for-hmm">The EM for HMM</h2>
<p>The  final question would be, given a set of observations, what are the values of the state transition probabilities A and the output emission probabilities B that make the data most likely.</p>

<ul>
  <li>The Naive application of EM to HMMs
<img src="http://om1hdizoc.bkt.clouddn.com/18-1-21/10373446.jpg" alt="" />
The derivation of the EM make use of Markov assumption (which can be proved by d-seperation in Bayesian network). Also it make use of Lagrange multiplier and Forward-Backward algorithms and a bunch of things to come up with the following variant of EM for HMM.</li>
  <li>The Forward-Backward algorithm for HMM parameter learning
<img src="http://om1hdizoc.bkt.clouddn.com/18-1-21/54629756.jpg" alt="" />
In some sense, $\gamma_t(i,j)$ can be computed statistically.  It is proportional to the probability of transitioning between state $s_i$ and $s_j$ at time $t$ given all of our observations $\vec{x}$.
    <blockquote>
      <p>Like many applications of EM, parameter learning for HMMs is a non-convex problem with many local maxima. EM will converge to a maximum based on its initial parameters, so multiple runs might be in order. Also, it is often important to smooth the probability distributions represented by A and B so that no transition or emission is assigned 0 probability. <a href="http://cs229.stanford.edu/section/cs229-hmm.pdf">[2]</a></p>
      <ul>
        <li>The last line refer to the log exp trick.</li>
      </ul>
    </blockquote>
  </li>
</ul>

<h2 id="relationship-to-kf-pf-and-other-time-series-estimator-model">Relationship to KF, PF, and other time series estimator model</h2>
<p>|State|Dynamics|Noise|Generalization|Cost|Model|
|:–:|:–|:–|:–|:–|:–:|
|Continuous/Discrete|Linear|Normal|Only Linear System|Cheapest|KF|
|Continuous/Discrete|Linear/None-linear|Normal|Depending on the order of Taylor series, the 2rd is arguably a better choice than UKF|Cheap|EKF|
|Continuous/Discrete|Linear/None-linear|Normal|Able to model non-linear system better than 1st order EKF|Less Expensive|UKF|
|Continuous/Discrete|Linear/None-linear|Any|Strong, should be able to model any dynamics and any distribution|Expensive, difficult to tune as well|PF|
|Discrete|Difficult to compare, the above filters uses state space model, while HMM uses probabilistic model|No assumption of Gaussian or linear what so ever|Time series of data or temporal data|Uses the forward-backward algorithm which is proportional to $O(|S|*T )$|HMM|</p>

<p>Some will say that KF or variants of KF is a special case of HMM. I hold my doubts about that since I haven’t read related papers yet.</p>

  </div>
  
  <!-- Related posts -->
  
  
  
    <div class="row related-posts">
      <h2 class="text-center" style="font-family: initial">Related blog posts:</h2>
      <div class="medium-12 small-12 columns">
        
          

           <h3>
            <a href="http://localhost:4000/blog/2018/06/04/ternary-and-one-hot-neuron.html">
              承接Binary, 拥抱ternary和one-hot-neurons
            </a>
           </h3>

          
        
          

           <h3>
            <a href="http://localhost:4000/blog/2018/05/29/kramdown_img_notes.html">
              Kramdown Image Tips
            </a>
           </h3>

          
        
          

           <h3>
            <a href="http://localhost:4000/blog/2018/05/29/bsn.html">
              Binary Stochastic Neurons
            </a>
           </h3>

          
        
      </div>
    </div>
  


  <!-- Disqus --><a class="u-url" href="/blog/2018/02/23/hidden-markov.html" hidden></a>
</article>


<script id="dsq-count-scr" src="//everitt257.disqus.com/count.js" async></script>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">Everitt&#39;s blog</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">Everitt&#39;s blog</li><li><a class="u-email" href="mailto:everitt257@gmail.com">everitt257@gmail.com</a></li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"><li><a href="https://github.com/everitt257"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg> <span class="username">everitt257</span></a></li></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>This site fancies machine learning and problems in engineerning in general.</p>
      </div>
    </div>

  </div>

</footer>
</body>

</html>
